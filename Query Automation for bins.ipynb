{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45ba6041-d99a-4d11-aaea-aced4b06b571",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Main function and Sub Functions\n",
    "Use for finding the Soft and Hard Bin counts when having Lot and Wafer Number and EG to Bin Counts. This function will helps you automatically finds the followings :\n",
    "1. designator\n",
    "2. scribe_family\n",
    "3. Bin_1 Counts\n",
    "4. Bin_2 Counts\n",
    "5. Bin_7 Counts\n",
    "6. Distinct Softbin Values\n",
    "7. Yield\n",
    "8. Yield_loss\n",
    "\n",
    "(For confidentail reason the numbers displayed are fake and some outputs are deleted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78afcd68-ccc7-4b43-b837-e18be7a54e3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "# Read the first CSV file and data engineering\n",
    "def eng_dic(input_path):\n",
    "  res = defaultdict(list)\n",
    "  df = pd.read_csv(input_path)\n",
    "  df.rename(columns={df.columns[0]: 'Lot', df.columns[1]: 'Wafer'}, inplace=True)\n",
    "  for i in range(len(df)):\n",
    "    res[str(df.loc[(i), 'Lot'])].append(df.loc[(i), 'Wafer']) \n",
    "  return res\n",
    "\n",
    "# sub function for query\n",
    "def Lot_wafer_to_family(lot, wafer_n):\n",
    "    Lot_wafer_number_to_family = f\"\"\"\n",
    "        with scribeNumber AS (\n",
    "        select WaferScribe, lot, WaferNumber from main.mes.vtmpfablotwaferdetails \n",
    "        where Site = 'DFW'\n",
    "    ),\n",
    "    -- gives scribed number count that both have\n",
    "\n",
    "    scribe_family_in_DFW AS (\n",
    "        Select distinct(ll.vcFamily) as scribe_family, SN.lot as lot, SN.WaferNumber as WaferNumber, VA.vcDesignator\n",
    "        FROM main.raw_yield.vsmtdata_all as VA\n",
    "        join main.raw_yield.vsmt_loadlot as ll\n",
    "        ON ll.vcLotCode = VA.vcLotCode\n",
    "        join scribeNumber as SN\n",
    "        ON SN.WaferScribe = VA.vcWaferID\n",
    "        where VA.vcWaferID = SN.WaferScribe\n",
    "    )\n",
    "    -- gives scribed number count that VA have\n",
    "    select scribe_family, vcDesignator\n",
    "    from scribe_family_in_DFW\n",
    "    Where lot = '{lot}' and  WaferNumber = {wafer_n}\n",
    "    -- test case HJFHF25008\tff1301E\t9\n",
    "    limit 10\n",
    "    ;\n",
    "    \"\"\"\n",
    "    return spark.sql(Lot_wafer_number_to_family)\n",
    "\n",
    "\n",
    "def querying_wafer_data(lot, wafer, family, designator):\n",
    "  aggregation_query = f'''\n",
    "  WITH base_query AS (\n",
    "    SELECT MES.ParentLot, MES.WaferNumber, a.Hardbin, a.{designator}, a.Barcode, a.Softbin\n",
    "    FROM main.testdw.{family}_finaltests AS a\n",
    "    INNER JOIN (\n",
    "        SELECT ParentLot, WaferScribe, WaferNumber\n",
    "        FROM main.mes.vfablotwaferdetails\n",
    "    ) AS MES ON MES.WaferScribe = a.{designator}\n",
    "    WHERE a.touchID = (\n",
    "        SELECT max(touchID)\n",
    "        FROM main.testdw.{family}_finaltests\n",
    "        WHERE Barcode = a.Barcode\n",
    "    )\n",
    "    AND mes.ParentLot = '{lot}'\n",
    "    AND mes.WaferNumber in ({wafer})\n",
    "),\n",
    "pivoted_data AS (\n",
    "    SELECT ParentLot, WaferNumber,\n",
    "    Count(DISTINCT(Softbin)) AS Distinct_Softbin_Values,\n",
    "           SUM(CASE WHEN Hardbin = 1 THEN 1 ELSE 0 END) AS H_Bin_1,\n",
    "           SUM(CASE WHEN Hardbin = 2 THEN 1 ELSE 0 END) AS H_Bin_2,\n",
    "           SUM(CASE WHEN Hardbin = 7 THEN 1 ELSE 0 END) AS H_Bin_7\n",
    "           -- SUM(CASE WHEN Softbin = 1 THEN 1 ELSE 0 END) AS S_Bin_1,\n",
    "           -- SUM(CASE WHEN Softbin = 2 THEN 1 ELSE 0 END) AS S_Bin_2,\n",
    "           -- SUM(CASE WHEN Softbin = 7 THEN 1 ELSE 0 END) AS S_Bin_7\n",
    "    FROM base_query\n",
    "    GROUP BY ParentLot, WaferNumber\n",
    ")\n",
    "SELECT ParentLot, WaferNumber, H_Bin_1, H_Bin_2, H_Bin_7, Distinct_Softbin_Values, Round(H_Bin_1/(H_Bin_1+H_Bin_2), 4) as Yield, Round(1-Yield, 4) as Yield_Loss\n",
    "FROM pivoted_data;'''\n",
    "\n",
    "  return spark.sql(aggregation_query)\n",
    "\n",
    "\n",
    "\n",
    "# Main function for generating output\n",
    "def lot_wafer_to_bin_finder(dicts_of_lots_and_wafer = None, output_path = None, input_path = None):\n",
    "  keep = True\n",
    "  table_list = []\n",
    "  while (keep):\n",
    "    try:\n",
    "      if dicts_of_lots_and_wafer:\n",
    "        for key, value in dicts_of_lots_and_wafer.items():\n",
    "          print(key, value)\n",
    "          for i in range(len(value)):\n",
    "              lot = key\n",
    "              wafer = value[i]\n",
    "              first_query = Lot_wafer_to_family(lot, wafer).collect()\n",
    "              scribe_family = first_query[0][0].lower()\n",
    "              designator = first_query[0][1]\n",
    "              #   designator = first_query.select('vcDesignator').collect()[0][0]\n",
    "              table = querying_wafer_data(lot, wafer, scribe_family, designator)\n",
    "              print(f'The lot number and wafer number belongs to {scribe_family}, and the bin is in the following:')\n",
    "              if output_path:\n",
    "                df = table.toPandas()\n",
    "                df.to_csv(f\"{output_path+lot+'-'+str(wafer)+str(i)}.csv\")\n",
    "              table.show()\n",
    "      elif input_path:\n",
    "        res_df = pd.DataFrame()\n",
    "        for key, value in eng_dic(input_path).items():\n",
    "          print(key, value)\n",
    "          for i in range(len(value)):\n",
    "              lot = key\n",
    "              wafer = value[i]\n",
    "              first_query = Lot_wafer_to_family(lot, wafer).collect()\n",
    "              scribe_family = first_query[0][0].lower()\n",
    "              designator = first_query[0][1]\n",
    "              #   designator = first_query.select('vcDesignator').collect()[0][0]\n",
    "              table = querying_wafer_data(lot, wafer, scribe_family, designator)\n",
    "              print(f'The lot number and wafer number belongs to {scribe_family}, and the bin is in the following:')\n",
    "              table.show()\n",
    "              if output_path:\n",
    "                df = table.toPandas()\n",
    "                res_df = pd.concat([res_df, df], ignore_index=True)\n",
    "        res_df.to_csv(f\"{output_path}combined.csv\")\n",
    "               \n",
    "      else:\n",
    "        lot = str(input('Please enter the lot number: '))\n",
    "        wafer = str(input('Please enter the wafer number: '))\n",
    "    #   EG = str(input('Please enter the EG(MesProduct for example: EG-M09558): '))\n",
    "        first_query = Lot_wafer_to_family(lot, wafer).collect()\n",
    "        scribe_family = first_query[0][0].lower()\n",
    "        designator = first_query[0][1]\n",
    "      #   designator = first_query.select('vcDesignator').collect()[0][0]\n",
    "        table = querying_wafer_data(lot, wafer, scribe_family, designator)\n",
    "        print(f'The lot number and wafer number belongs to {scribe_family}, and the bin is in the following:')\n",
    "        table.show()\n",
    "        if output_path:\n",
    "          df = table.toPandas()\n",
    "          # display(df.select(\"*\"))\n",
    "          df.to_csv(f\"{output_path+lot+'-'+str(wafer)}.csv\")\n",
    "      stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "      if stay == 0:\n",
    "        keep = False\n",
    "    except Exception as e:\n",
    "      print(f\"There is no data associated with lot {lot} and wafer {wafer} or no finaltest table for family {scribe_family} or you might already have the output CSV. Please try again.\")\n",
    "      stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "      if stay == 0:\n",
    "        keep = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8689ff6-bb1c-4136-8d2f-c136237dd729",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#1:  Single lot-wafer\n",
    "\n",
    "Syntax: \n",
    "- lot_wafer_to_bin_finder()                                     when you only want to see the output\n",
    "- lot_wafer_to_bin_finder(output_path = /path/to/your/folder)   when you want to have csv as your output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c2ec345-6260-48ab-b2da-ea8b612c10bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#2:  List of lots-wafers\n",
    "lots_and_wafer = {  \n",
    "  2403242: [1,2,3]   \n",
    "  2418766: [1,5,7]     \n",
    "}\n",
    "\n",
    "Syntax: \n",
    "- lot_wafer_to_bin_finder(lots_and_wafer) when you only want to see the output\n",
    "- lot_wafer_to_bin_finder(lots_and_wafer, output_path = '/path/to/your/folder') when you don't want input csv and want multiple output csv\n",
    "- lot_wafer_to_bin_finder( input_path = 'path/to/your/csv/file', output_path = '/path/to/your/folder') when you want to have 1 csv as your input and output\n",
    "\n",
    "#### Note: the input CSV must have lot at the first column and wafer number at the second column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4165c88-56c9-4e75-adf4-c153a03dd338",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#3:  Single Lot  (analyze all wafers in lot)\n",
    "syntax: \n",
    "- lot_to_bin_finder() when you only want to see the output\n",
    "- lot_to_bin_finder(output_path = 'your/path') when you want a output csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2a7fb7-f494-4b2a-9560-3f5f2212b823",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_c1(path):\n",
    "  df = pd.read_csv(path)\n",
    "  df= list(df.iloc[: ,0])\n",
    "  return df\n",
    "\n",
    "#functions for task 3\n",
    "def Lot_to_family(lot):\n",
    "    Lot_to_family = f\"\"\"\n",
    "        with scribeNumber AS (\n",
    "        select WaferScribe, lot, WaferNumber from main.mes.vtmpfablotwaferdetails \n",
    "        where Site = 'DFW'\n",
    "    ),\n",
    "    -- gives scribed number count that both have\n",
    "\n",
    "    scribe_family_in_DFW AS (\n",
    "        Select distinct(ll.vcFamily) as scribe_family, SN.lot as lot, SN.WaferNumber as WaferNumber, VA.vcDesignator\n",
    "        FROM main.raw_yield.vsmtdata_all as VA\n",
    "        join main.raw_yield.vsmt_loadlot as ll\n",
    "        ON ll.vcLotCode = VA.vcLotCode\n",
    "        join scribeNumber as SN\n",
    "        ON SN.WaferScribe = VA.vcWaferID\n",
    "        where VA.vcWaferID = SN.WaferScribe\n",
    "    )\n",
    "    -- gives scribed number count that VA have\n",
    "    select scribe_family, vcDesignator, WaferNumber\n",
    "    from scribe_family_in_DFW\n",
    "    Where lot = '{lot}'\n",
    "    -- test case QQ654666\t2211301E\t9\n",
    "    limit 10\n",
    "    ;\n",
    "    \"\"\"\n",
    "    return spark.sql(Lot_to_family)\n",
    "\n",
    "\n",
    "def querying_wafer_data(lot, wafer, family, designator):\n",
    "  aggregation_query = f'''\n",
    "  WITH base_query AS (\n",
    "    SELECT MES.ParentLot, MES.WaferNumber, a.Hardbin, a.{designator}, a.Barcode, a.Softbin\n",
    "    FROM main.testdw.{family}_finaltests AS a\n",
    "    INNER JOIN (\n",
    "        SELECT ParentLot, WaferScribe, WaferNumber\n",
    "        FROM main.mes.vfablotwaferdetails\n",
    "    ) AS MES ON MES.WaferScribe = a.{designator}\n",
    "    WHERE a.touchID = (\n",
    "        SELECT max(touchID)\n",
    "        FROM main.testdw.{family}_finaltests\n",
    "        WHERE Barcode = a.Barcode\n",
    "    )\n",
    "    AND mes.ParentLot = '{lot}'\n",
    "    AND mes.WaferNumber in ({wafer})\n",
    "),\n",
    "pivoted_data AS (\n",
    "    SELECT ParentLot, WaferNumber,\n",
    "    Count(DISTINCT(Softbin)) AS Distinct_Softbin_Values,\n",
    "           SUM(CASE WHEN Hardbin = 1 THEN 1 ELSE 0 END) AS H_Bin_1,\n",
    "           SUM(CASE WHEN Hardbin = 2 THEN 1 ELSE 0 END) AS H_Bin_2,\n",
    "           SUM(CASE WHEN Hardbin = 7 THEN 1 ELSE 0 END) AS H_Bin_7\n",
    "    FROM base_query\n",
    "    GROUP BY ParentLot, WaferNumber\n",
    ")\n",
    "SELECT ParentLot, WaferNumber, H_Bin_1, H_Bin_2, H_Bin_7, Distinct_Softbin_Values, Round(H_Bin_1/(H_Bin_1+H_Bin_2), 4) as Yield, Round(1-Yield, 4) as Yield_Loss\n",
    "FROM pivoted_data;'''\n",
    "\n",
    "  return spark.sql(aggregation_query)\n",
    "\n",
    "  # Main function for generating output\n",
    "def lot_to_bin_finder(lots = None, input_path = None, output_path=None):\n",
    "  keep = True\n",
    "  table_list = []\n",
    "  if input_path:\n",
    "    lots = extract_c1(input_path)\n",
    "  while (keep):\n",
    "    try:\n",
    "      if lots:\n",
    "        res_df = pd.DataFrame() \n",
    "        for lot in lots:\n",
    "          first_query = Lot_to_family(lot).collect()\n",
    "          scribe_family = first_query[0][0].lower()\n",
    "          designator = first_query[0][1]\n",
    "          list_of_wafernumber = []\n",
    "          for i in range(len(first_query)):\n",
    "            list_of_wafernumber.append(first_query[i][2])\n",
    "          string_wafer_ns = [str(wafern) for wafern in list_of_wafernumber]\n",
    "          result_wafer = \", \".join(string_wafer_ns)\n",
    "          #   designator = first_query.select('vcDesignator').collect()[0][0]\n",
    "          table = querying_wafer_data(lot, result_wafer, scribe_family, designator)\n",
    "          print(f'The lot number and wafer number belongs to {scribe_family}, and the bin is in the following:')\n",
    "          table.show()\n",
    "          if output_path:\n",
    "            df = table.toPandas()\n",
    "            res_df = pd.concat([res_df, df], ignore_index=True)\n",
    "        res_df.to_csv(f\"{output_path}combined_lot_bin.csv\")\n",
    "          \n",
    "      else:\n",
    "        lot = str(input('Please enter the lot number: '))\n",
    "        first_query = Lot_to_family(lot).collect()\n",
    "        scribe_family = first_query[0][0].lower()\n",
    "        designator = first_query[0][1]\n",
    "        list_of_wafernumber = []\n",
    "        for i in range(len(first_query)):\n",
    "          list_of_wafernumber.append(first_query[i][2])\n",
    "        string_wafer_ns = [str(wafern) for wafern in list_of_wafernumber]\n",
    "        result_wafer = \", \".join(string_wafer_ns)\n",
    "        #   designator = first_query.select('vcDesignator').collect()[0][0]\n",
    "        table = querying_wafer_data(lot, result_wafer, scribe_family, designator)\n",
    "        print(f'The lot number and wafer number belongs to {scribe_family}, and the bin is in the following:')\n",
    "        table.show()\n",
    "        if output_path:\n",
    "          df = table.toPandas()\n",
    "          df.to_csv(f\"{output_path+lot}.csv\")\n",
    "      stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "      if stay == 0:\n",
    "        keep = False\n",
    "    except Exception as e:\n",
    "      print(f\"There is no data associated with lot {lot} or no finaltest table for family {scribe_family}. Please try again.\")\n",
    "      lots = lots[1:]\n",
    "      stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "      if stay == 0:\n",
    "        keep = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a31da39a-58f6-491e-87d3-7ab3b2b630c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Please enter the lot number:  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:103)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2(SequenceExecutionState.scala:103)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2$adapted(SequenceExecutionState.scala:100)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:100)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:720)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:439)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:439)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1297)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:1023)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:979)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:683)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:709)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:708)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:763)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:556)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:549)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:549)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:527)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:107)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:107)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:89)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:103)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2(SequenceExecutionState.scala:103)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2$adapted(SequenceExecutionState.scala:100)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:100)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:720)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:439)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:439)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1297)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:1023)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:979)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:683)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:709)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:708)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:763)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:556)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:549)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:549)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:527)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:107)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:107)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:89)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lot_to_bin_finder() # for just output\n",
    "lot_to_bin_finder(output_path='/Volumes/scratch/walt_wu/jason/') # for output CSV\n",
    "\n",
    "# example output:\n",
    "# Please enter the lot number:\n",
    "# 2129838\n",
    "# The lot number and wafer number belongs to qm77180, and the bin is in the following:\n",
    "# +---------+-----------+-------+-------+-------+-----------------------+------------------+--------------------+\n",
    "# |ParentLot|WaferNumber|H_Bin_1|H_Bin_2|H_Bin_7|Distinct_Softbin_Values|             Yield|          Yield_Loss|\n",
    "# +---------+-----------+-------+-------+-------+-----------------------+------------------+--------------------+\n",
    "# |       1|          6|  52483|   4361|      0|                      2|0.9232812609950038| 0.07671873900499615|\n",
    "# |       2|          2|  34044|   5566|      0|                      2|0.8594799293107801|  0.1405200706892199|\n",
    "# |       3|          5|  16297|    848|      0|                     38|0.9505395158938466|0.049460484106153446|\n",
    "# |       4|          4|  46222|   2421|      0|                     41|0.9502292210595563| 0.04977077894044368|\n",
    "# |       5|          3|  22772|   2342|      0|                     57|0.9067452416978578| 0.09325475830214225|\n",
    "# +---------+-----------+-------+-------+-------+-----------------------+------------------+--------------------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32c7bc9d-ded0-46c2-8483-668ae1031e0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#4:  Multiple Lots  (analyze all wafers in all lots)\n",
    "\n",
    "syntax:  \n",
    "\n",
    "-   lots = ['2129838', '2129839']  \n",
    "lot_to_bin_finder(lots)\n",
    "Do the above when you specified lots on databrick\n",
    "- lots = ['2129838', '2129839']  \n",
    "lot_to_bin_finder(lots, output_path = 'path/to/output/folder/') when you want to save result as an csv\n",
    "- lot_to_bin_finder(input_path = 'path/to/your/csv/file', output_path = '/path/to/your/folder/') when you want to have 1 csv as your input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "449f264c-ebf0-44a6-802d-025d7e689102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:438)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1297)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:1023)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:979)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:683)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:709)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:708)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:763)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:556)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:549)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:549)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:527)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:107)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:107)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:89)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:438)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1297)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:1023)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:979)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:683)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:709)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:708)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:763)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:556)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:549)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:549)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:527)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:107)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:107)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:89)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lots = ['2129838', '2129838']\n",
    "# lot_to_bin_finder(lots) #for output only\n",
    "lot_to_bin_finder(input_path='/Volumes/scratch/walt_wu/jason/2129838.csv', output_path='/Volumes/scratch/walt_wu/jason/') \n",
    "\n",
    "# example:\n",
    "# The lot number and wafer number belongs to qm77180, and the bin is in the following:\n",
    "# +---------+-----------+-------+-------+-------+-------+-------+-------+------------------+--------------------+\n",
    "# |ParentLot|WaferNumber|H_Bin_1|H_Bin_2|H_Bin_7|S_Bin_1|S_Bin_2|S_Bin_7|             Yield|          Yield_Loss|\n",
    "# +---------+-----------+-------+-------+-------+-------+-------+-------+------------------+--------------------+\n",
    "# |  1234567|          6|  52483|   4361|      0|  52483|      0|      0|0.9232812609950038| 0.07671873900499615|\n",
    "# |  1234567|          2|  34044|   5566|      0|  34044|      0|      0|0.8594799293107801|  0.1405200706892199|\n",
    "# |  1234567|          5|  16297|    848|      0|  16297|      0|      0|0.9505395158938466|0.049460484106153446|\n",
    "# |  1234567|          4|  46222|   2421|      0|  46222|      0|      0|0.9502292210595563| 0.04977077894044368|\n",
    "# |  1234567|          3|  22772|   2342|      0|  22772|      0|      0|0.9067452416978578| 0.09325475830214225|\n",
    "# +---------+-----------+-------+-------+-------+-------+-------+-------+------------------+--------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca169f2e-cc35-48f9-ac23-6a0c5b7bcbc4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. EG to find lot wafer number and Bin\n",
    "syntax:  \n",
    "- EG_to_bin_finder()  When you only want the output\n",
    "- EGs = ['22222', '33333', '44444']  \n",
    "EG_to_bin_finder(EGs)  when you want to run multiple EGs and output as csv\n",
    "- EG_to_bin_finder(input='input/path', output_path= 'output/path/)  \n",
    "when you want input and output both as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "997c9e04-2843-45b7-bead-2d0ff97a2991",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def extract_c1(path):\n",
    "  df = pd.read_csv(path)\n",
    "  df= list(df.iloc[: ,0])\n",
    "  return df\n",
    "\n",
    "\n",
    "#functions for task 5\n",
    "def EG_to_family(EG):\n",
    "    Lot_to_family = f\"\"\"\n",
    "        with scribeNumber AS (\n",
    "        select WaferScribe, lot, WaferNumber\n",
    "        from main.mes.vtmpfablotwaferdetails \n",
    "        where Site = 'DFW'\n",
    "    ),\n",
    "    -- gives scribed number count that both have\n",
    "\n",
    "    scribe_family_in_DFW AS (\n",
    "        Select distinct(ll.vcFamily) as scribe_family, SN.lot as lot, SN.WaferNumber as WaferNumber, VA.vcDesignator, VA.vcWaferMaterial as EG\n",
    "        FROM main.raw_yield.vsmtdata_all as VA\n",
    "        join main.raw_yield.vsmt_loadlot as ll\n",
    "        ON ll.vcLotCode = VA.vcLotCode\n",
    "        join scribeNumber as SN\n",
    "        ON SN.WaferScribe = VA.vcWaferID\n",
    "        where VA.vcWaferID = SN.WaferScribe\n",
    "    )\n",
    "    -- gives scribed number count that VA have\n",
    "    select scribe_family, vcDesignator, WaferNumber, EG, lot\n",
    "    from scribe_family_in_DFW\n",
    "    where EG = '{EG}'\n",
    "    -- test case QM25008\t2211301E\t9\n",
    "    ;\n",
    "    \"\"\"\n",
    "    return spark.sql(Lot_to_family)\n",
    "\n",
    "\n",
    "def EG_querying_wafer_data(family, designator, EG):\n",
    "  aggregation_query = f'''\n",
    "  WITH base_query AS (\n",
    "    SELECT MES.ParentLot, MES.WaferNumber, a.Hardbin, a.{designator}, a.Barcode, a.Softbin\n",
    "    FROM main.testdw.{family}_finaltests AS a\n",
    "    INNER JOIN (\n",
    "        SELECT ParentLot, WaferScribe, WaferNumber\n",
    "        FROM main.mes.vfablotwaferdetails\n",
    "    ) AS MES ON MES.WaferScribe = a.{designator}\n",
    "    WHERE a.touchID = (\n",
    "        SELECT max(touchID)\n",
    "        FROM main.testdw.{family}_finaltests\n",
    "        WHERE Barcode = a.Barcode\n",
    "    )\n",
    "),\n",
    "pivoted_data AS (\n",
    "    SELECT ParentLot, WaferNumber,\n",
    "    Count(DISTINCT(Softbin)) AS Distinct_Softbin_Values,\n",
    "           SUM(CASE WHEN Hardbin = 1 THEN 1 ELSE 0 END) AS H_Bin_1,\n",
    "           SUM(CASE WHEN Hardbin = 2 THEN 1 ELSE 0 END) AS H_Bin_2,\n",
    "           SUM(CASE WHEN Hardbin = 7 THEN 1 ELSE 0 END) AS H_Bin_7\n",
    "    FROM base_query\n",
    "    GROUP BY ParentLot, WaferNumber\n",
    ")\n",
    "SELECT ParentLot, WaferNumber, H_Bin_1, H_Bin_2, H_Bin_7, Distinct_Softbin_Values, Round(H_Bin_1/(H_Bin_1+H_Bin_2), 4) as Yield, Round(1-Yield, 4) as Yield_Loss\n",
    "FROM pivoted_data;\n",
    ";'''\n",
    "\n",
    "  return spark.sql(aggregation_query)\n",
    "\n",
    "\n",
    "\n",
    "# Main function for generating output\n",
    "def EG_to_bin_finder(EGs = None, input_path = None, output_path=None):\n",
    "  keep = True\n",
    "  table_list = []\n",
    "  if input_path:\n",
    "    EGs = extract_c1(input_path)\n",
    "  while (keep):\n",
    "    try:\n",
    "      if EGs:\n",
    "        res_df = pd.DataFrame()\n",
    "        for EG in EGs:\n",
    "          first_query = EG_to_family(EG).collect()\n",
    "        family_des = defaultdict(set)\n",
    "        for i in range(len(first_query)):\n",
    "          if first_query[i][0]:\n",
    "            if first_query[i][0].lower() in family_des:\n",
    "              family_des[first_query[i][0].lower()].add(first_query[i][1])\n",
    "            else:\n",
    "              family_des[first_query[i][0].lower()] = {first_query[i][1]}\n",
    "          else:\n",
    "            continue\n",
    "      for key, value in family_des.items():\n",
    "        for designator in list(value):\n",
    "          print(key, designator, EG)\n",
    "          table = EG_querying_wafer_data(key, designator, EG)\n",
    "          print(f'The EG query belongs to family {key} and designator {designator}, and the bin is in the following:')\n",
    "          table.show()\n",
    "          if output_path:\n",
    "            df = table.toPandas()\n",
    "            res_df = pd.concat([res_df, df], ignore_index=True)\n",
    "        res_df.to_csv(f\"{output_path}combined_EG_bin.csv\")\n",
    "\n",
    "\n",
    "      else:\n",
    "        EG = str(input('Please enter the EG(MesProduct for example: EG-M09558): '))\n",
    "        family_des = defaultdict(set)\n",
    "        first_query = EG_to_family(EG).collect()\n",
    "        family_des = defaultdict(set)\n",
    "        for i in range(len(first_query)):\n",
    "          if first_query[i][0]:\n",
    "            if first_query[i][0].lower() in family_des:\n",
    "              family_des[first_query[i][0].lower()].add(first_query[i][1])\n",
    "            else:\n",
    "              family_des[first_query[i][0].lower()] = {first_query[i][1]}\n",
    "          else:\n",
    "            continue\n",
    "      for key, value in family_des.items():\n",
    "        for designator in list(value):\n",
    "          print(key, designator, EG)\n",
    "          table = EG_querying_wafer_data(key, designator, EG)\n",
    "          print(f'The EG query belongs to family {key} and designator {designator}, and the bin is in the following:')\n",
    "          table.show()\n",
    "          \n",
    "      stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "      if stay == 0:\n",
    "        keep = False\n",
    "    except Exception as e:\n",
    "      print(f\"There is no data associated with EG {EG}. Please try again.\")\n",
    "      stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "      if stay == 0:\n",
    "        keep = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fac43134-4d35-4643-97ec-83d336610139",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# V.2 functions\n",
    "\n",
    "## Having some updates on V1 query result and ability to download query result into CSVs  \n",
    "- Query output no longer have softbin 1,2,7 counts, instead adding count of distinct softbins\n",
    "- Rounding yield and yield loss to the 4th decimals\n",
    "- Functions new ability to enter the path and download query result into CSVs\n",
    "\n",
    "#### Note: all the V2 functions' syntax is the same as V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea8d540-4496-4212-acb5-8f93a2939335",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "# Read the first CSV file and data engineering\n",
    "def eng_dic(input_path):\n",
    "  res = defaultdict(list)\n",
    "  df = pd.read_csv(input_path)\n",
    "  df.rename(columns={df.columns[0]: 'Lot', df.columns[1]: 'Wafer'}, inplace=True)\n",
    "  for i in range(len(df)):\n",
    "    res[str(df.loc[(i), 'Lot'])].append(df.loc[(i), 'Wafer']) \n",
    "  return res\n",
    "\n",
    "# sub function for query\n",
    "def Lot_wafer_to_family_v2(lot, wafer_n):\n",
    "    Lot_wafer_number_to_family = f\"\"\"\n",
    "        with scribeNumber AS (\n",
    "        select WaferScribe, lot, WaferNumber from main.mes.vtmpfablotwaferdetails \n",
    "        where Site = 'DFW'\n",
    "    ),\n",
    "    -- gives scribed number count that both have\n",
    "\n",
    "    scribe_family_in_DFW AS (\n",
    "        Select distinct(ll.vcFamily) as scribe_family, SN.lot as lot, SN.WaferNumber as WaferNumber, VA.vcDesignator, VA.vcWaferMaterial as EG\n",
    "        FROM main.raw_yield.vsmtdata_all as VA\n",
    "        join main.raw_yield.vsmt_loadlot as ll\n",
    "        ON ll.vcLotCode = VA.vcLotCode\n",
    "        join scribeNumber as SN\n",
    "        ON SN.WaferScribe = VA.vcWaferID\n",
    "        where VA.vcWaferID = SN.WaferScribe\n",
    "    )\n",
    "    -- gives scribed number count that VA have\n",
    "    select scribe_family, vcDesignator, EG\n",
    "    from scribe_family_in_DFW\n",
    "    Where lot = '{lot}' and  WaferNumber = {wafer_n};\n",
    "    -- test case QM25008\t2211301E\t9\n",
    "    \"\"\"\n",
    "    return spark.sql(Lot_wafer_number_to_family)\n",
    "\n",
    "\n",
    "def querying_wafer_data_v2(lot, wafer, family, designator, EG):\n",
    "  aggregation_query = f'''\n",
    "  WITH base_query AS (\n",
    "    SELECT MES.ParentLot, MES.WaferNumber, a.Hardbin, a.{designator}, a.Barcode, a.Softbin\n",
    "    FROM main.testdw.{family}_finaltests AS a\n",
    "    INNER JOIN (\n",
    "        SELECT ParentLot, WaferScribe, WaferNumber\n",
    "        FROM main.mes.vfablotwaferdetails\n",
    "    ) AS MES ON MES.WaferScribe = a.{designator}\n",
    "    WHERE a.touchID = (\n",
    "        SELECT max(touchID)\n",
    "        FROM main.testdw.{family}_finaltests\n",
    "        WHERE Barcode = a.Barcode\n",
    "    )\n",
    "    AND mes.ParentLot = '{lot}'\n",
    "    AND mes.WaferNumber in ({wafer})\n",
    "),\n",
    "pivoted_data AS (\n",
    "    SELECT \n",
    "    '{family}' as Family,\n",
    "    '{EG}' as EG,\n",
    "    ParentLot, \n",
    "    WaferNumber,\n",
    "    Hardbin as HardBin_Type,\n",
    "    Softbin as SoftBin_Type,\n",
    "    Count(*) AS Module_Count\n",
    "    FROM base_query\n",
    "    GROUP BY ParentLot, WaferNumber, Hardbin, Softbin\n",
    ")\n",
    "SELECT Family, EG, ParentLot, WaferNumber, HardBin_Type, SoftBin_Type, Module_Count\n",
    "FROM pivoted_data;'''\n",
    "\n",
    "  return spark.sql(aggregation_query)\n",
    "\n",
    "\n",
    "\n",
    "# Main function for generating output\n",
    "def lot_wafer_to_bin_finder_v2(dicts_of_lots_and_wafer = None, output_path = None, input_path = None):\n",
    "  keep = True\n",
    "  table_list = []\n",
    "  if input_path:\n",
    "    dicts_of_lots_and_wafer = eng_dic(input_path)\n",
    "  while (keep):\n",
    "    # try:\n",
    "    if dicts_of_lots_and_wafer:\n",
    "      for key, value in dicts_of_lots_and_wafer.items():\n",
    "        print(key, value)\n",
    "        for i in range(len(value)):\n",
    "            lot = key\n",
    "            wafer = value[i]\n",
    "            first_query = Lot_wafer_to_family_v2(lot, wafer).collect()\n",
    "            scribe_family = first_query[0][0].lower()\n",
    "            designator = first_query[0][1]\n",
    "            EG = first_query[0][2]\n",
    "            #   designator = first_query.select('vcDesignator').collect()[0][0]\n",
    "            table = querying_wafer_data_v2(lot, wafer, scribe_family, designator, EG)\n",
    "            print(f'The lot number and wafer number belongs to {scribe_family}, and the bin is in the following:')\n",
    "            if output_path:\n",
    "              df = table.toPandas()\n",
    "              df.to_csv(f\"{output_path+lot+'-'+str(wafer)+str(i)}.csv\")\n",
    "            table.show()\n",
    "    else:\n",
    "      lot = str(input('Please enter the lot number: '))\n",
    "      wafer = str(input('Please enter the wafer number: '))\n",
    "  #   EG = str(input('Please enter the EG(MesProduct for example: EG-M09558): '))\n",
    "      first_query = Lot_wafer_to_family_v2(lot, wafer).collect()\n",
    "      scribe_family = first_query[0][0].lower()\n",
    "      designator = first_query[0][1]\n",
    "      EG = first_query[0][2]\n",
    "      #   designator = first_query.select('vcDesignator').collect()[0][0]\n",
    "      table = querying_wafer_data_v2(lot, wafer, scribe_family, designator, EG)\n",
    "      print(f'The lot number and wafer number belongs to {scribe_family}, and the bin is in the following:')\n",
    "      table.show()\n",
    "      if output_path:\n",
    "        df = table.toPandas()\n",
    "        # display(df.select(\"*\"))\n",
    "        df.to_csv(f\"{output_path+lot+'-'+str(wafer)}.csv\")\n",
    "    stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "    if stay == 0:\n",
    "      keep = False\n",
    "    # except Exception as e:\n",
    "    #   print(f\"There is no data associated with lot {lot} and wafer {wafer} or no finaltest table for family {scribe_family} or you might already have the output CSV. Please try again.\")\n",
    "    #   stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "    #   if stay == 0:\n",
    "    #     keep = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ec30b35-9cb3-4e7d-9cf4-4e8571d93f9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#1:  Single lot-wafer\n",
    "#2:  Input list of lot and wafers (Through direct input and CSV)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f3bd924-ea31-46e2-a463-12e663513665",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3: Single lot \n",
    "# 4: Multiple Lots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea29375-3e62-44f9-9429-7e173cf44794",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_c1(path):\n",
    "  df = pd.read_csv(path)\n",
    "  df= list(df.iloc[: ,0])\n",
    "  return df\n",
    "\n",
    "#functions for task 3\n",
    "def Lot_to_family_v2(lot):\n",
    "    Lot_to_family = f\"\"\"\n",
    "        with scribeNumber AS (\n",
    "        select WaferScribe, lot, WaferNumber from main.mes.vtmpfablotwaferdetails \n",
    "        where Site = 'DFW'\n",
    "    ),\n",
    "    -- gives scribed number count that both have\n",
    "\n",
    "    scribe_family_in_DFW AS (\n",
    "        Select distinct(ll.vcFamily) as scribe_family, SN.lot as lot, SN.WaferNumber as WaferNumber, VA.vcDesignator, VA.vcWaferMaterial as EG\n",
    "        FROM main.raw_yield.vsmtdata_all as VA\n",
    "        join main.raw_yield.vsmt_loadlot as ll\n",
    "        ON ll.vcLotCode = VA.vcLotCode\n",
    "        join scribeNumber as SN\n",
    "        ON SN.WaferScribe = VA.vcWaferID\n",
    "        where VA.vcWaferID = SN.WaferScribe\n",
    "    )\n",
    "    -- gives scribed number count that VA have\n",
    "    select scribe_family, vcDesignator, WaferNumber, EG\n",
    "    from scribe_family_in_DFW\n",
    "    Where lot = '{lot}'\n",
    "    -- test case QM25008\t2211301E\t9\n",
    "    limit 10\n",
    "    ;\n",
    "    \"\"\"\n",
    "    return spark.sql(Lot_to_family)\n",
    "\n",
    "\n",
    "def querying_wafer_data_v2(lot, wafer, family, designator, EG):\n",
    "  aggregation_query = f'''\n",
    "  WITH base_query AS (\n",
    "    SELECT MES.ParentLot, MES.WaferNumber, a.Hardbin, a.{designator}, a.Barcode, a.Softbin\n",
    "    FROM main.testdw.{family}_finaltests AS a\n",
    "    INNER JOIN (\n",
    "        SELECT ParentLot, WaferScribe, WaferNumber\n",
    "        FROM main.mes.vfablotwaferdetails\n",
    "    ) AS MES ON MES.WaferScribe = a.{designator}\n",
    "    WHERE a.touchID = (\n",
    "        SELECT max(touchID)\n",
    "        FROM main.testdw.{family}_finaltests\n",
    "        WHERE Barcode = a.Barcode\n",
    "    )\n",
    "    AND mes.ParentLot = '{lot}'\n",
    "    AND mes.WaferNumber in ({wafer})\n",
    "),\n",
    "pivoted_data AS (\n",
    "    SELECT \n",
    "    '{family}' as Family,\n",
    "    '{EG}' as EG,\n",
    "    ParentLot, \n",
    "    WaferNumber,\n",
    "    Hardbin as HardBin_Type,\n",
    "    Softbin as SoftBin_Type,\n",
    "    Count(*) AS Module_Count\n",
    "    FROM base_query\n",
    "    GROUP BY ParentLot, WaferNumber, Hardbin, Softbin\n",
    ")\n",
    "SELECT Family, EG, ParentLot, WaferNumber, HardBin_Type, SoftBin_Type, Module_Count\n",
    "FROM pivoted_data;'''\n",
    "\n",
    "  return spark.sql(aggregation_query)\n",
    "\n",
    "  # Main function for generating output\n",
    "def lot_to_bin_finder_v2(lots = None, input_path = None, output_path=None):\n",
    "  keep = True\n",
    "  table_list = []\n",
    "  if input_path:\n",
    "    lots = extract_c1(input_path)\n",
    "  while (keep):\n",
    "    # try:\n",
    "    if lots:\n",
    "      res_df = pd.DataFrame() \n",
    "      for lot in lots:\n",
    "        first_query = Lot_to_family_v2(lot).collect()\n",
    "        scribe_family = first_query[0][0].lower()\n",
    "        designator = first_query[0][1]\n",
    "        EG = first_query[0][3]\n",
    "        print(EG)\n",
    "        list_of_wafernumber = []\n",
    "        for i in range(len(first_query)):\n",
    "          list_of_wafernumber.append(first_query[i][2])\n",
    "        string_wafer_ns = [str(wafern) for wafern in list_of_wafernumber]\n",
    "        result_wafer = \", \".join(string_wafer_ns)\n",
    "        #   designator = first_query.select('vcDesignator').collect()[0][0]\n",
    "        table = querying_wafer_data_v2(lot, result_wafer, scribe_family, designator, EG)\n",
    "        print(f'The lot number and wafer number belongs to {scribe_family}, and the bin is in the following:')\n",
    "        table.show()\n",
    "        if output_path:\n",
    "          df = table.toPandas()\n",
    "          res_df = pd.concat([res_df, df], ignore_index=True)\n",
    "      res_df.to_csv(f\"{output_path}combined_lot_bin.csv\")\n",
    "        \n",
    "    else:\n",
    "      lot = str(input('Please enter the lot number: '))\n",
    "      first_query = Lot_to_family_v2(lot).collect()\n",
    "      scribe_family = first_query[0][0].lower()\n",
    "      designator = first_query[0][1]\n",
    "      EG = first_query[0][3]\n",
    "      list_of_wafernumber = []\n",
    "      for i in range(len(first_query)):\n",
    "        list_of_wafernumber.append(first_query[i][2])\n",
    "      string_wafer_ns = [str(wafern) for wafern in list_of_wafernumber]\n",
    "      result_wafer = \", \".join(string_wafer_ns)\n",
    "      #   designator = first_query.select('vcDesignator').collect()[0][0]\n",
    "      table = querying_wafer_data_v2(lot, result_wafer, scribe_family, designator, EG)\n",
    "      print(f'The lot number and wafer number belongs to {scribe_family}, and the bin is in the following:')\n",
    "      table.show()\n",
    "      if output_path:\n",
    "        df = table.toPandas()\n",
    "        df.to_csv(f\"{output_path+lot}_bin.csv\")\n",
    "    stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "    if stay == 0:\n",
    "      keep = False\n",
    "    # except Exception as e:\n",
    "    #   print(f\"There is no data associated with lot {lot} or no finaltest table for family {scribe_family}. Please try again.\")\n",
    "    #   lots = lots[1:]\n",
    "    #   stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "    #   if stay == 0:\n",
    "    #     keep = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58d0e671-3bc3-43db-abb9-abb6aa3a6113",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5: EG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66966f79-5f8d-4585-99db-de06b5a96db7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def extract_c1(path):\n",
    "  df = pd.read_csv(path)\n",
    "  df= list(df.iloc[: ,0])\n",
    "  return df\n",
    "\n",
    "\n",
    "#functions for task 5\n",
    "def EG_to_family_v2(EG):\n",
    "    Lot_to_family = f\"\"\"\n",
    "        with scribeNumber AS (\n",
    "        select WaferScribe, lot, WaferNumber\n",
    "        from main.mes.vtmpfablotwaferdetails \n",
    "        where Site = 'DFW'\n",
    "    ),\n",
    "    -- gives scribed number count that both have\n",
    "\n",
    "    scribe_family_in_DFW AS (\n",
    "        Select distinct(ll.vcFamily) as scribe_family, SN.lot as lot, SN.WaferNumber as WaferNumber, VA.vcDesignator, VA.vcWaferMaterial as EG\n",
    "        FROM main.raw_yield.vsmtdata_all as VA\n",
    "        join main.raw_yield.vsmt_loadlot as ll\n",
    "        ON ll.vcLotCode = VA.vcLotCode\n",
    "        join scribeNumber as SN\n",
    "        ON SN.WaferScribe = VA.vcWaferID\n",
    "        where VA.vcWaferID = SN.WaferScribe\n",
    "    )\n",
    "    -- gives scribed number count that VA have\n",
    "    select scribe_family, vcDesignator, WaferNumber, EG, lot\n",
    "    from scribe_family_in_DFW\n",
    "    where EG = '{EG}'\n",
    "    -- test case QM25008\t2211301E\t9\n",
    "    ;\n",
    "    \"\"\"\n",
    "    return spark.sql(Lot_to_family)\n",
    "\n",
    "\n",
    "def EG_querying_wafer_data_v2(family, designator, EG):\n",
    "  aggregation_query = f'''\n",
    "  WITH base_query AS (\n",
    "    SELECT MES.ParentLot, MES.WaferNumber, a.Hardbin, a.{designator}, a.Barcode, a.Softbin\n",
    "    FROM main.testdw.{family}_finaltests AS a\n",
    "    INNER JOIN (\n",
    "        SELECT ParentLot, WaferScribe, WaferNumber\n",
    "        FROM main.mes.vfablotwaferdetails\n",
    "    ) AS MES ON MES.WaferScribe = a.{designator}\n",
    "    WHERE a.touchID = (\n",
    "        SELECT max(touchID)\n",
    "        FROM main.testdw.{family}_finaltests\n",
    "        WHERE Barcode = a.Barcode\n",
    "    )\n",
    "),\n",
    "pivoted_data AS (\n",
    "    SELECT \n",
    "    '{family}' as Family,\n",
    "    '{EG}' as EG,\n",
    "    ParentLot, \n",
    "    WaferNumber,\n",
    "    Hardbin as HardBin_Type,\n",
    "    Softbin as SoftBin_Type,\n",
    "    Count(*) AS Module_Count\n",
    "    FROM base_query\n",
    "    GROUP BY ParentLot, WaferNumber, Hardbin, Softbin\n",
    ")\n",
    "SELECT Family, EG, ParentLot, WaferNumber, HardBin_Type, SoftBin_Type, Module_Count\n",
    "FROM pivoted_data;'''\n",
    "\n",
    "  return spark.sql(aggregation_query)\n",
    "\n",
    "\n",
    "\n",
    "# Main function for generating output\n",
    "def EG_to_bin_finder_v2(EGs = None, input_path = None, output_path=None):\n",
    "  keep = True\n",
    "  table_list = []\n",
    "  if input_path:\n",
    "    EGs = extract_c1(input_path)\n",
    "  while (keep):\n",
    "    try:\n",
    "      if EGs:\n",
    "        res_df = pd.DataFrame()\n",
    "        for EG in EGs:\n",
    "          first_query = EG_to_family_v2(EG).collect()\n",
    "        family_des = defaultdict(set)\n",
    "        for i in range(len(first_query)):\n",
    "          if first_query[i][0]:\n",
    "            if first_query[i][0].lower() in family_des:\n",
    "              family_des[first_query[i][0].lower()].add(first_query[i][1])\n",
    "            else:\n",
    "              family_des[first_query[i][0].lower()] = {first_query[i][1]}\n",
    "          else:\n",
    "            continue\n",
    "      for key, value in family_des.items():\n",
    "        for designator in list(value):\n",
    "          print(key, designator, EG)\n",
    "          table = EG_querying_wafer_data_v2(key, designator, EG)\n",
    "          print(f'The EG query belongs to family {key} and designator {designator}, and the bin is in the following:')\n",
    "          table.show()\n",
    "          if output_path:\n",
    "            df = table.toPandas()\n",
    "            res_df = pd.concat([res_df, df], ignore_index=True)\n",
    "        res_df.to_csv(f\"{output_path}combined_EG_bin.csv\")\n",
    "\n",
    "\n",
    "      else:\n",
    "        EG = str(input('Please enter the EG(MesProduct for example: EG-M09558): '))\n",
    "        family_des = defaultdict(set)\n",
    "        first_query = EG_to_family_v2(EG).collect()\n",
    "        family_des = defaultdict(set)\n",
    "        for i in range(len(first_query)):\n",
    "          if first_query[i][0]:\n",
    "            if first_query[i][0].lower() in family_des:\n",
    "              family_des[first_query[i][0].lower()].add(first_query[i][1])\n",
    "            else:\n",
    "              family_des[first_query[i][0].lower()] = {first_query[i][1]}\n",
    "          else:\n",
    "            continue\n",
    "      for key, value in family_des.items():\n",
    "        for designator in list(value):\n",
    "          print(key, designator, EG)\n",
    "          table = EG_querying_wafer_data_v2(key, designator, EG)\n",
    "          print(f'The EG query belongs to family {key} and designator {designator}, and the bin is in the following:')\n",
    "          table.show()\n",
    "          \n",
    "      stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "      if stay == 0:\n",
    "        keep = False\n",
    "    except Exception as e:\n",
    "      print(f\"There is no data associated with EG {EG}. Please try again.\")\n",
    "      stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "      if stay == 0:\n",
    "        keep = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b835d0e-d66e-4668-b93a-50a1c435d86d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2c73ad6-e9c1-45a5-ac83-fd153e4f7d57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Some notes for myself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcdd07d4-e3c0-4d4f-bb4f-57d801f9eddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "aggregation_query_test = '''\n",
    "WITH base_query AS (\n",
    "    SELECT MES.ParentLot, MES.WaferNumber, a.Hardbin, a.U10, a.Barcode, a.Softbin\n",
    "    FROM main.testdw.qm77180_finaltests AS a\n",
    "    INNER JOIN (\n",
    "        SELECT ParentLot, WaferScribe, WaferNumber\n",
    "        FROM main.mes.vfablotwaferdetails\n",
    "    ) AS MES ON MES.WaferScribe = a.U14\n",
    "    WHERE a.touchID = (\n",
    "        SELECT max(touchID)\n",
    "        FROM main.testdw.qm77180_finaltests\n",
    "        WHERE Barcode = a.Barcode\n",
    "    )\n",
    "),\n",
    "pivoted_data AS (\n",
    "    SELECT ParentLot, WaferNumber,\n",
    "    Count(DISTINCT(Softbin)) AS Distinct_Softbin_Values,\n",
    "           SUM(CASE WHEN Hardbin = 1 THEN 1 ELSE 0 END) AS H_Bin_1,\n",
    "           SUM(CASE WHEN Hardbin = 2 THEN 1 ELSE 0 END) AS H_Bin_2,\n",
    "           SUM(CASE WHEN Hardbin = 7 THEN 1 ELSE 0 END) AS H_Bin_7\n",
    "           -- SUM(CASE WHEN Softbin = 1 THEN 1 ELSE 0 END) AS S_Bin_1,\n",
    "           -- SUM(CASE WHEN Softbin = 2 THEN 1 ELSE 0 END) AS S_Bin_2,\n",
    "           -- SUM(CASE WHEN Softbin = 7 THEN 1 ELSE 0 END) AS S_Bin_7\n",
    "    FROM base_query\n",
    "    GROUP BY ParentLot, WaferNumber\n",
    ")\n",
    "SELECT ParentLot, WaferNumber, H_Bin_1, H_Bin_2, H_Bin_7, Distinct_Softbin_Values, Round(H_Bin_1/(H_Bin_1+H_Bin_2), 4) as Yield, Round(1-Yield, 4) as Yield_Loss\n",
    "FROM pivoted_data;\n",
    "'''\n",
    "\n",
    "\n",
    "df = spark.sql(aggregation_query_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9182fa9b-b15f-45b9-8bed-5d2fa1777ced",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eng_dic('/Volumes/scratch/walt_wu/jason/Databricks Module Report mockups  2024_0604a.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc64909-c51f-4fa9-bf5a-bf530fa67172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb6d3d6-b1fc-464f-b2c8-538839eb3461",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "  FROM (\n",
    "      SELECT MES.ParentLot, MES.WaferNumber, a.Hardbin, a.FL3439, a.Barcode\n",
    "      FROM main.testdw.qm77298_finaltests AS a\n",
    "      INNER JOIN (\n",
    "          SELECT ParentLot, WaferScribe, WaferNumber\n",
    "          FROM main.mes.vfablotwaferdetails\n",
    "      ) AS MES ON MES.WaferScribe = a.FL3439\n",
    "      WHERE a.touchID = (\n",
    "          SELECT max(touchID)\n",
    "          FROM main.testdw.qm77298_finaltests\n",
    "          WHERE Barcode = a.Barcode\n",
    "      )\n",
    "          AND mes.ParentLot = '2324406'\n",
    "        AND mes.WaferNumber = 1\n",
    "  ) subquery\n",
    "  PIVOT (\n",
    "      COUNT(Barcode)\n",
    "      FOR Hardbin IN (1, 2, 7)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d1b4f7f-28c9-4243-84e9-5e76ffc2ca0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def querying_family_data(lot):\n",
    "  find_family = f'''Select Distinct(LL.vcFamily)\n",
    "from main.raw_yield.vsmt_loadlot LL\n",
    "join main.raw_yield.vsmtdata_all A\n",
    "on LL.vcLotCode = A.vcLotCode\n",
    "WHERE LL.vcLotCode = '{lot}'\n",
    "limit 10;'''\n",
    "  return spark.sql(find_family)\n",
    "\n",
    "def family_to_data(family):\n",
    "  aggregation_query = f'''SELECT *\n",
    "  FROM (\n",
    "      SELECT MES.ParentLot, MES.WaferNumber, a.Hardbin, a.FL3439, a.Barcode\n",
    "      FROM main.testdw.{family}_finaltests AS a\n",
    "      INNER JOIN (\n",
    "          SELECT ParentLot, WaferScribe, WaferNumber\n",
    "          FROM main.mes.vfablotwaferdetails\n",
    "      ) AS MES ON MES.WaferScribe = a.FL3439\n",
    "      WHERE a.touchID = (\n",
    "          SELECT max(touchID)\n",
    "          FROM main.testdw.{family}_finaltests\n",
    "          WHERE Barcode = a.Barcode\n",
    "      )\n",
    "        --  AND mes.ParentLot = '{lot}'\n",
    "        -- AND mes.WaferNumber = {wafer}\n",
    "  ) subquery\n",
    "  PIVOT (\n",
    "      COUNT(Barcode)\n",
    "      FOR Hardbin IN (1, 2, 7)\n",
    "  );'''\n",
    "\n",
    "  return spark.sql(aggregation_query)\n",
    "\n",
    "def EG_to_bin_finder():\n",
    "  keep = True\n",
    "  while (keep):\n",
    "    try:\n",
    "      lot = str(input('Please enter the lot number: '))\n",
    "      family = querying_family_data(lot).select('vcFamily').collect()[0][0]\n",
    "      table = querying_wafer_data(lot, wafer, EG)\n",
    "      table.show()\n",
    "      stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "      if stay == 0:\n",
    "        keep = False\n",
    "    except Exception as e:\n",
    "      print(f\"There is no data associated with lot {lot}, wafer {wafer}, and EG {EG}. Please try again.\")\n",
    "      stay = int(input('If you would like to stop querying enter 0 \\nIf you would like to continue querying enter 1: '))\n",
    "      if stay == 0:\n",
    "        keep = False\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd454009-e900-4ba5-8134-70c61d6d9704",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first_query = EG_to_family('WEG9197').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5512dc-628e-4bf7-9216-a6140bbddacc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(first_query[561][0])\n",
    "family_des = defaultdict(set)\n",
    "for i in range(len(first_query)):\n",
    "  if first_query[i][0]:\n",
    "    if first_query[i][0].lower() in family_des:\n",
    "      family_des[first_query[i][0].lower()].add(first_query[i][1])\n",
    "    else:\n",
    "      family_des[first_query[i][0].lower()] = {first_query[i][1]}\n",
    "  else:\n",
    "    continue\n",
    "print(family_des)\n",
    "print('here2222')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e6879e-317c-44dc-941b-0ba137efdf1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for key, value in family_des.items():\n",
    "      for designator in list(value):\n",
    "        print(key, designator)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "62e18551-233e-4c21-aa6e-6a5dfbe6c442",
     "origId": 4240529797712547,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Query Automation for bins",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
